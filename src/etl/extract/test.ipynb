{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_direct_to_minio.py\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"a\":[3,3,3], \"b\":[\"x\",\"y\",\"z\"]})\n",
    "\n",
    "df.to_parquet(\n",
    "    \"s3://mlflow/raw/df.parquet\",\n",
    "    index=False,\n",
    "    storage_options={\n",
    "        \"key\": \"minio\",\n",
    "        \"secret\": \"minio12345\",\n",
    "        \"client_kwargs\": {\"endpoint_url\": \"http://localhost:9002\"}\n",
    "    },\n",
    ")\n",
    "print(\"‚úÖ wrote to s3://mlflow/raw/df.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28157ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"dvc\", \"update\", \"dvc_meta/df.parquet.dvc\"], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50799a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc.api\n",
    "import pandas as pd\n",
    "\n",
    "with dvc.api.open(\n",
    "    \"dvc_meta/df.parquet\",   # ‡∏à‡∏∞‡πÉ‡∏™‡πà path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á ‡∏´‡∏£‡∏∑‡∏≠ .dvc ‡∏Å‡πá‡πÑ‡∏î‡πâ\n",
    "    repo=\".\",\n",
    "    remote=\"storage\",\n",
    "    mode=\"rb\"                # üëà ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö parquet\n",
    ") as f:\n",
    "    df = pd.read_parquet(f)  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ pyarrow ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc.api\n",
    "import pandas as pd\n",
    "\n",
    "with dvc.api.open(\n",
    "    \"dvc_meta/newsseg_desc_not_null.csv\",   # ‡πÉ‡∏ä‡πâ path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠ .dvc ‡∏Å‡πá‡πÑ‡∏î‡πâ\n",
    "    repo=\".\",\n",
    "    remote=\"storage\", \n",
    "    mode=\"r\",            # text mode\n",
    "    encoding=\"utf-8\"     # ‡∏•‡∏≠‡∏á \"utf-8-sig\" ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á error\n",
    ") as f:\n",
    "    df = pd.read_csv(f)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0991c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.etl.extract.data_structure_extract_strategy import DataExtractor\n",
    "\n",
    "# cfg_minio_csv = {\n",
    "#     \"type\": \"minio\",\n",
    "#     \"endpoint\": \"localhost:9002\",\n",
    "#     \"access_key\": \"minio\",\n",
    "#     \"secret_key\": \"minio12345\",\n",
    "#     \"bucket\": \"test\",\n",
    "#     \"object\": \"test/myfile.csv\",\n",
    "#     \"secure\": False,\n",
    "#     \"encoding\": \"utf-8\",\n",
    "# }\n",
    "\n",
    "# extractor_csv = DataExtractor.get_extractor(cfg=cfg_minio_csv)\n",
    "# data_list = extractor_csv.extractor()\n",
    "# for name, df in data_list:\n",
    "#     print(\"üìÇ\", name, type(df))\n",
    "#     x= df.head()\n",
    "\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á client\n",
    "client = Minio(\n",
    "    \"localhost:9002\",         # üëà ‡πÉ‡∏ä‡πâ‡∏û‡∏≠‡∏£‡πå‡∏ï API ‡∏ó‡∏µ‡πà map ‡πÑ‡∏ß‡πâ\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"minio12345\",\n",
    "    secure=False,             # ‡∏ñ‡πâ‡∏≤ http ‡πÉ‡∏´‡πâ False, https ‡πÉ‡∏´‡πâ True\n",
    ")\n",
    "\n",
    "bucket = \"test\"\n",
    "object_name = \"test/myfile.csv\"  # ‡∏ä‡∏∑‡πà‡∏≠ object ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡πÄ‡∏õ‡πä‡∏∞\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î object\n",
    "response = client.get_object(bucket, object_name)\n",
    "\n",
    "try:\n",
    "    data = response.read()  # ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥\n",
    "finally:\n",
    "    response.close()\n",
    "    response.release_conn()\n",
    "\n",
    "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå\n",
    "ext = Path(object_name).suffix.lower()\n",
    "if ext == \".parquet\":\n",
    "    df = pd.read_parquet(BytesIO(data))  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á pyarrow\n",
    "elif ext == \".csv\":\n",
    "    df = pd.read_csv(BytesIO(data), encoding=\"utf-8\")\n",
    "elif ext == \".json\":\n",
    "    df = pd.read_json(BytesIO(data))\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "\n",
    "client = Minio(\n",
    "    \"localhost:9002\", access_key=\"minio\", secret_key=\"minio12345\", secure=False\n",
    ")\n",
    "# client.remove_bucket(\"test\")\n",
    "client.fput_object(\n",
    "    \"test\",\n",
    "    \"test/myfile.csv\",  # object name (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô bucket)\n",
    "    \"C:/Users/Lenovo/Desktop/AI-News-Project/data/interim/data_news_segmentation/description_not_null/newsseg_desc_not_null_v20250811_144642.csv\",  # path ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á\n",
    ")\n",
    "print(\"Upload done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742599eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from minio import Minio\n",
    "\n",
    "project_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "\n",
    "class BaseStructureExtractor(ABC):\n",
    "    @abstractmethod\n",
    "    def extractor(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "class DbExtractor(BaseStructureExtractor):\n",
    "    def __init__(self, conn_str: str, table: str = None, query: str = None):\n",
    "        if table is None and query is None:\n",
    "            raise ValueError(\n",
    "                \"Either 'table' or 'query' must be provided for DbExtractor.\"\n",
    "            )\n",
    "        self.conn_str = conn_str\n",
    "        self.table = table\n",
    "        self.query = query\n",
    "        self.engine = None\n",
    "        self.conn = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.engine = create_engine(self.conn_str)\n",
    "        self.conn = self.engine.connect()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.conn is not None:\n",
    "            self.conn.close()\n",
    "        if self.engine is not None:\n",
    "            self.engine.dispose()\n",
    "\n",
    "    def extractor(self) -> pd.DataFrame:\n",
    "        if self.conn is None:\n",
    "            self.engine = create_engine(self.conn_str)\n",
    "            with self.engine.connect() as conn:\n",
    "                if self.query:\n",
    "                    return pd.read_sql(self.query, conn)\n",
    "                else:\n",
    "                    return pd.read_sql_table(self.table, conn)\n",
    "        else:\n",
    "            if self.query:\n",
    "                return pd.read_sql(self.query, self.conn)\n",
    "            else:\n",
    "                return pd.read_sql_table(self.table, self.conn)\n",
    "\n",
    "\n",
    "class CsvExtractor(BaseStructureExtractor):\n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def extractor(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.filepath)\n",
    "\n",
    "\n",
    "class SourceBBCLocalExtractor(BaseStructureExtractor):\n",
    "    def __init__(self, filepath: str):\n",
    "        if filepath is None:\n",
    "            self.filepath = os.path.join(\n",
    "                project_path,\n",
    "                \"data\",\n",
    "                \"raw\",\n",
    "                \"source_pariza_sharif_BBC_news_summary\",\n",
    "                \"BBC News Summary\",\n",
    "            )\n",
    "        else:\n",
    "            self.filepath = filepath\n",
    "\n",
    "    def extractor(self) -> pd.DataFrame:\n",
    "        data = []\n",
    "        for root, _, files in os.walk(self.filepath):\n",
    "            parts = Path(root).parts\n",
    "            if len(parts) >= 2 and parts[-2] in {\"News Articles\", \"Summaries\"}:\n",
    "                article_type = parts[-2]\n",
    "                category = parts[-1]\n",
    "                for filename in files:\n",
    "                    full_path = os.path.join(root, filename)\n",
    "                    try:\n",
    "                        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            content = f.read()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not read file {full_path}. Error: {e}\")\n",
    "                        content = \"\"\n",
    "                    data.append(\n",
    "                        {\n",
    "                            \"filename\": filename,\n",
    "                            \"category\": category,\n",
    "                            \"article_or_summary\": article_type,\n",
    "                            \"text_content\": content,\n",
    "                        }\n",
    "                    )\n",
    "        df = pd.DataFrame(data)\n",
    "        if df.empty:\n",
    "            return df\n",
    "        df = df.pivot_table(\n",
    "            index=[\"filename\", \"category\"],\n",
    "            columns=\"article_or_summary\",\n",
    "            values=\"text_content\",\n",
    "            aggfunc=\"first\",\n",
    "        ).reset_index()\n",
    "        df.columns.name = None\n",
    "        for col in [\"News Articles\", \"Summaries\"]:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        return df[[\"filename\", \"category\", \"News Articles\", \"Summaries\"]]\n",
    "\n",
    "\n",
    "class DvcExtractor(BaseStructureExtractor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        repo: str = \".\",\n",
    "        remote: str = \"storage\",\n",
    "        encoding: str = \"utf-8\",\n",
    "        rev: str = None,\n",
    "    ):\n",
    "        self.path = path\n",
    "        self.repo = repo\n",
    "        self.remote = remote\n",
    "        self.encoding = encoding\n",
    "        self.rev = rev\n",
    "\n",
    "    def extractor(self) -> pd.DataFrame:\n",
    "        import dvc.api\n",
    "\n",
    "        ext = Path(self.path).suffix.lower()\n",
    "        mode = \"rb\" if ext == \".parquet\" else \"r\"\n",
    "\n",
    "        with dvc.api.open(\n",
    "            self.path,\n",
    "            repo=self.repo,\n",
    "            remote=self.remote,\n",
    "            mode=mode,\n",
    "            encoding=None if ext == \".parquet\" else self.encoding,\n",
    "            rev=self.rev,\n",
    "        ) as f:\n",
    "            if ext == \".parquet\":\n",
    "                return pd.read_parquet(f)\n",
    "            elif ext == \".csv\":\n",
    "                return pd.read_csv(f)\n",
    "            elif ext == \".json\":\n",
    "                return pd.read_json(f)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "\n",
    "\n",
    "class MinioDataFrameExtractor(BaseStructureExtractor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        access_key: str,\n",
    "        secret_key: str,\n",
    "        bucket: str,\n",
    "        object_name: str,\n",
    "        secure: bool = False,\n",
    "        encoding: str = \"utf-8\",\n",
    "        version_id: str | None = None,\n",
    "        region: str | None = None,\n",
    "        recursive: bool = True,\n",
    "    ):\n",
    "        self.client = Minio(\n",
    "            endpoint,\n",
    "            access_key=access_key,\n",
    "            secret_key=secret_key,\n",
    "            secure=secure,\n",
    "            region=region,\n",
    "        )\n",
    "        self.bucket = bucket\n",
    "        self.object_name = object_name\n",
    "        self.encoding = encoding\n",
    "        self.version_id = version_id\n",
    "        self.recursive = recursive\n",
    "\n",
    "    def extractor(self) -> list:\n",
    "        results = []\n",
    "\n",
    "        try:\n",
    "            self.client.stat_object(\n",
    "                self.bucket, self.object_name, version_id=self.version_id\n",
    "            )\n",
    "            object_names = [self.object_name]\n",
    "        except Exception:\n",
    "            object_names = [\n",
    "                obj.object_name\n",
    "                for obj in self.client.list_objects(\n",
    "                    self.bucket, prefix=self.object_name, recursive=self.recursive\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        for name in object_names:\n",
    "            ext = Path(name).suffix.lower()\n",
    "            resp = self.client.get_object(self.bucket, name, version_id=self.version_id)\n",
    "            try:\n",
    "                data = resp.read()\n",
    "            finally:\n",
    "                try:\n",
    "                    resp.close()\n",
    "                    resp.release_conn()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if ext == \".parquet\":\n",
    "                df = pd.read_parquet(BytesIO(data))\n",
    "            elif ext == \".csv\":\n",
    "                df = pd.read_csv(BytesIO(data), encoding=self.encoding)\n",
    "            elif ext == \".json\":\n",
    "                df = pd.read_json(BytesIO(data))\n",
    "            else:\n",
    "                df = data  # unstructured ‚Üí ‡∏Ñ‡∏∑‡∏ô raw bytes ‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô\n",
    "\n",
    "            results.append((name, df))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class DataExtractor:\n",
    "    @staticmethod\n",
    "    def get_extractor(cfg: dict) -> BaseStructureExtractor:\n",
    "        try:\n",
    "            if cfg[\"type\"] == \"db\":\n",
    "                return DbExtractor(cfg[\"conn_str\"], cfg[\"table\"], cfg.get(\"query\"))\n",
    "            elif cfg[\"type\"] == \"csv\":\n",
    "                return CsvExtractor(cfg[\"path\"])\n",
    "            elif cfg[\"type\"] == \"local_bbc\":\n",
    "                return SourceBBCLocalExtractor(cfg[\"path\"])\n",
    "            elif cfg[\"type\"] == \"dvc\":\n",
    "                return DvcExtractor(\n",
    "                    cfg[\"path\"], cfg[\"repo\"], cfg[\"remote\"], cfg[\"encoding\"], cfg[\"rev\"]\n",
    "                )\n",
    "            elif cfg[\"type\"].lower() == \"minio\":  # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö minIO/Minio/minio\n",
    "                return MinioDataFrameExtractor(\n",
    "                    endpoint=cfg[\"endpoint\"],\n",
    "                    access_key=cfg[\"access_key\"],\n",
    "                    secret_key=cfg[\"secret_key\"],\n",
    "                    bucket=cfg[\"bucket\"],\n",
    "                    object_name=cfg[\"object\"],\n",
    "                    secure=cfg.get(\"secure\", False),\n",
    "                    encoding=cfg.get(\"encoding\", \"utf-8\"),\n",
    "                    version_id=cfg.get(\"version_id\"),\n",
    "                    region=cfg.get(\"region\"),\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported Extractor type: {cfg['type']}\")\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Configuration is missing required key: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5818cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join('../../..')))\n",
    "# from src.etl.load.data_load_strategy import CrossDbUpsertLoader\n",
    "\n",
    "\n",
    "# loader = CrossDbUpsertLoader(\n",
    "#     conn_str=\"postgresql://postgres:!NTpatn2549@localhost:5433/postgres\",  # ‡∏´‡∏£‡∏∑‡∏≠ mysql+pymysql://..., sqlite:///...\n",
    "#     table=\"mas_news\",\n",
    "#     schema=\"public\",                      # SQLite ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á\n",
    "#     conflict_cols=[\"id\"], # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ UNIQUE/PK ‡πÉ‡∏ô DB\n",
    "#     do_update=False,                       # True=‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï / False=‡∏Ç‡πâ‡∏≤‡∏°\n",
    "#     update_cols=None,       # None=‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏µ‡∏¢‡πå\n",
    "# )\n",
    "# loader.load(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ca1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine, MetaData, Table\n",
    "\n",
    "# engine = create_engine(\"postgresql://postgres:!NTpatn2549@localhost:5433/postgres\", echo=True)\n",
    "\n",
    "# # 1) ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata container\n",
    "# metadata = MetaData()\n",
    "\n",
    "# # 2) Reflect ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\n",
    "# users = Table(\"mas_news\", metadata, autoload_with=engine)\n",
    "\n",
    "# # 3) Inspect column\n",
    "# print(users.c.keys())   # ['id', 'name', 'email']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871392d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine, MetaData, Table\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "\n",
    "# engine = create_engine(\"postgresql://postgres:!NTpatn2549@localhost:5433/postgres\", future=True)\n",
    "# md = MetaData()\n",
    "# tbl = Table(\"mas_news\", md, schema=\"public\", autoload_with=engine)\n",
    "# df = pd.DataFrame([\n",
    "#     {\"url\":\"https://a1.com\",\"publishedtime\":\"2025-05-28 18:40:20.000\",\"titlenews\":\"A1\",\"source\":\"BBC\"},\n",
    "#     {\"url\":\"https://a2.com\",\"publishedtime\":\"2025-05-28 18:40:20.000\",\"titlenews\":\"A1-new\",\"source\":\"BBC\"},  # ‡∏ä‡∏ô\n",
    "# ])\n",
    "# records = df.to_dict(orient=\"records\")\n",
    "# print(records)\n",
    "# stmt = insert(tbl).values(records)\n",
    "# stmt = stmt.on_conflict_do_update(\n",
    "#     index_elements=[tbl.c.url, tbl.c.publishedtime],\n",
    "#     set_={\n",
    "#         \"titlenews\": stmt.excluded.titlenews,\n",
    "#         \"source\": stmt.excluded.source,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# with engine.begin() as con:\n",
    "#     con.execute(stmt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
